name: "Application"

on:
  push:
    branches: ["main"]
    paths:
      - "app/**"
      - "k8s/**"
      - ".github/**"

permissions:
  id-token: write
  contents: read
  security-events: write

jobs:
  build-and-deploy:
    name: "Build and Deploy"
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Run Gosec Security Scanner
        uses: securego/gosec@master
        with:
          args: ./app/...
        continue-on-error: true

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsProvisionerRole
          aws-region: us-west-2

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build Docker image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: wiz-exercise-todo-app
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG -t $ECR_REGISTRY/$ECR_REPOSITORY:latest ./app/tasky-main

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ steps.login-ecr.outputs.registry }}/wiz-exercise-todo-app:${{ github.sha }}
          format: "sarif"
          output: "trivy-results.sarif"

      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: "trivy-results.sarif"

      - name: Push Docker image to Amazon ECR
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: wiz-exercise-todo-app
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: "latest"

      - name: Install AWS SSM Session Manager Plugin
        run: |
          curl "https://s3.amazonaws.com/session-manager-downloads/plugin/latest/ubuntu_64bit/session-manager-plugin.deb" -o "session-manager-plugin.deb"
          sudo dpkg -i session-manager-plugin.deb

      - name: Get Bastion Instance ID and EKS Endpoint
        id: get-bastion-eks-info
        run: |
          # Get bastion instance ID
          BASTION_INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=wiz-exercise-bastion" "Name=instance-state-name,Values=running" \
            --query "Reservations[].Instances[].InstanceId" \
            --output text | head -n 1)

          if [ -z "$BASTION_INSTANCE_ID" ]; then
            echo "Error: Bastion instance not found or not running."
            exit 1
          fi
          echo "BASTION_INSTANCE_ID=$BASTION_INSTANCE_ID" >> $GITHUB_ENV

          # Get EKS Cluster Endpoint
          EKS_ENDPOINT=$(aws eks describe-cluster \
            --name wiz-exercise-eks \
            --query "cluster.endpoint" \
            --output text)

          if [ -z "$EKS_ENDPOINT" ]; then
            echo "Error: EKS cluster endpoint not found."
            exit 1
          fi
          echo "EKS_ENDPOINT=$EKS_ENDPOINT" >> $GITHUB_ENV

      - name: Start SSM Port Forwarding to EKS
        run: |
          EKS_ENDPOINT_CLEAN=$(echo ${{ env.EKS_ENDPOINT }} | sed 's|^https://||')
          echo "Forwarding to EKS endpoint: $EKS_ENDPOINT_CLEAN via ${{ env.BASTION_INSTANCE_ID }}"
          # Run in background, store PID
          aws ssm start-session \
            --target "${{ env.BASTION_INSTANCE_ID }}" \
            --document-name AWS-StartPortForwardingSessionToRemoteHost \
            --parameters "host=$EKS_ENDPOINT_CLEAN,portNumber=443,localPortNumber=6443" &
          SSM_SESSION_PID=$!
          echo "SSM_SESSION_PID=$SSM_SESSION_PID" >> $GITHUB_ENV
          echo "SSM Port forwarding session started with PID: $SSM_SESSION_PID"
          sleep 10 # Give the tunnel some time to establish

      - name: Update Kubeconfig for Port Forwarding
        run: |
          # Use the EKS endpoint directly for initial kubeconfig setup
          aws eks update-kubeconfig --name wiz-exercise-eks --region us-west-2

          # Modify kubeconfig to use the local forwarded port
          kubectl config set-cluster arn:aws:eks:us-west-2:${{ secrets.AWS_ACCOUNT_ID }}:cluster/wiz-exercise-eks \
            --server=https://localhost:6443 \
            --insecure-skip-tls-verify=true

          echo "Kubeconfig updated to use local port-forwarding (localhost:6443)."

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Install Helm
        uses: azure/setup-helm@v4
        with:
          version: "latest"

      - name: Get LB Controller Role ARN
        id: get-lb-role-arn
        run: |
          # Need to init Terraform to read outputs
          terraform -chdir=terraform init -upgrade
          LB_ROLE_ARN=$(terraform -chdir=terraform output -raw lb_controller_role_arn)
          VPC_ID=$(terraform -chdir=terraform output -raw vpc_id)
          echo "LB_ROLE_ARN=$LB_ROLE_ARN" >> $GITHUB_ENV
          echo "VPC_ID=$VPC_ID" >> $GITHUB_ENV
          echo "Retrieved LB Controller Role ARN: $LB_ROLE_ARN"
          echo "Retrieved VPC ID: $VPC_ID"

      - name: Deploy AWS Load Balancer Controller
        run: |
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            --namespace kube-system \
            --set clusterName=wiz-exercise-eks \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${{ env.LB_ROLE_ARN }}" \
            --set image.repository=602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon/aws-load-balancer-controller \
            --set region=us-west-2 \
            --set vpcId=${{ env.VPC_ID }} \
            --set installCRDs=true # Ensure CRDs are installed by Helm

      - name: Deploy to EKS
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          # 1. Get infrastructure info
          # 1. Get MongoDB Private IP
          # We filter by the Name tag and ensure it's 'running' to avoid getting IPs of terminated instances.
          MONGODB_IP=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=wiz-exercise-mongodb" "Name=instance-state-name,Values=running" \
            --query "Reservations[].Instances[].PrivateIpAddress" \
            --output text | head -n 1)

          echo "Found MongoDB at: $MONGODB_IP"

          # 2. Get MongoDB Password from Secrets Manager
          # Secrets Manager often appends a random suffix to names in Terraform.
          # This find-and-fetch logic ensures we get the right one.
          SECRET_NAME=$(aws secretsmanager list-secrets \
            --query "SecretList[?starts_with(Name, 'wiz-exercise/mongodb-auth')].Name" \
            --output text | awk '{print $1}')

          SECRET_JSON=$(aws secretsmanager get-secret-value \
            --secret-id "$SECRET_NAME" \
            --query SecretString --output text)

          MONGODB_PASSWORD=$(echo "$SECRET_JSON" | jq -r .password)
          MONGODB_USER=$(echo "$SECRET_JSON" | jq -r .username)

          # 2. Create namespace FIRST
          kubectl apply -f k8s/namespace.yaml

          # 3. Create secret (namespace now exists)
          kubectl create secret generic mongodb-secret \
            --namespace wiz-exercise \
            --from-literal=password="$MONGODB_PASSWORD" \
            --from-literal=username="$MONGODB_USER" \
            --dry-run=client -o yaml | kubectl apply -f -

          # 4. Replace placeholders
          #
          # Fix: Use | instead of / to avoid URI conflicts
          sed -i "s|AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com|${ECR_REGISTRY}|g" k8s/deployment.yaml
          # Ensure you also tag with the specific SHA instead of just 'latest' for better tracking
          sed -i "s|:latest|:${{ github.sha }}|g" k8s/deployment.yaml

          sed -i "s/MONGODB_HOST_IP/${MONGODB_IP}/g" k8s/mongodb-external.yaml
          sed -i "s/MONGODB_HOST_IP/${MONGODB_IP}/g" k8s/networkpolicy.yaml

          # 5. Apply remaining manifests in dependency order
          kubectl apply -f k8s/serviceaccount.yaml
          kubectl apply -f k8s/mongodb-external.yaml
          kubectl apply -f k8s/clusterrolebinding.yaml
          kubectl apply -f k8s/service.yaml
          kubectl apply -f k8s/networkpolicy.yaml
          kubectl apply -f k8s/pdb.yaml
          kubectl apply -f k8s/deployment.yaml
          kubectl apply -f k8s/ingress.yaml

      - name: Verify Deployment
        run: |
          # Show pod status
          kubectl get pods -n wiz-exercise

          # Show service and ingress
          kubectl get svc,ingress -n wiz-exercise

      - name: Get Application URL
        run: |
          echo "Waiting for ALB to be provisioned (this may take 2-3 minutes)..."

          ALB_URL=$(kubectl get ingress todo-app-ingress -n wiz-exercise -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

          if [ -z "$ALB_URL" ]; then
            echo "⚠️  ALB not ready yet. Check ingress status with: kubectl describe ingress -n wiz-exercise"
          else
            echo "✅ Application available at: http://$ALB_URL"
          fi

      - name: Terminate SSM Port Forwarding Session
        if: always() # Run always, even if previous steps fail
        run: |
          if [ -n "${{ env.SSM_SESSION_PID }}" ]; then
            echo "Terminating SSM session with PID: ${{ env.SSM_SESSION_PID }}"
            kill ${{ env.SSM_SESSION_PID }} || true
            echo "SSM session terminated."
          else
            echo "No active SSM session PID found to terminate."
          fi
